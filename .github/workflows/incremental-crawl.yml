name: Incremental Crawl

on:
  schedule:
    - cron: "0 0,6,12,18 * * *"  # Every 6 hours
  workflow_dispatch:
    inputs:
      force_full:
        description: "Force full crawl instead of incremental"
        type: boolean
        default: false

concurrency:
  group: incremental-crawl
  cancel-in-progress: true

env:
  CRAWL_MODE: ${{ inputs.force_full && 'full' || 'incremental' }}

jobs:
  # ─────────────────────────────────────────────
  # Phase 1: Incremental crawl — all registries
  # ─────────────────────────────────────────────

  crawl:
    runs-on: ubuntu-latest
    timeout-minutes: 60
    strategy:
      fail-fast: false
      matrix:
        include:
          - name: clawhub
            module: crawlers.clawhub
          - name: skills-sh
            module: crawlers.skills_sh
          - name: mcp-registry
            module: crawlers.mcp_registry
          - name: mcp-so
            module: crawlers.mcp_so
          - name: lobehub
            module: crawlers.lobehub

    steps:
      - uses: actions/checkout@v4

      - uses: actions/setup-python@v5
        with:
          python-version: "3.12"
          cache: pip

      - run: pip install -r requirements.txt

      - name: Restore crawl cache
        uses: actions/cache@v4
        with:
          path: data/${{ matrix.name }}/
          key: ${{ matrix.name }}-incr-${{ github.run_number }}
          restore-keys: |
            ${{ matrix.name }}-incr-
            ${{ matrix.name }}-

      - name: Crawl ${{ matrix.name }} (mode=${{ env.CRAWL_MODE }})
        env:
          TURSO_DATABASE_URL: ${{ secrets.TURSO_DATABASE_URL }}
          TURSO_AUTH_TOKEN: ${{ secrets.TURSO_AUTH_TOKEN }}
          GH_TOKEN: ${{ github.token }}
          PULSEMCP_API_KEY: ${{ secrets.PULSEMCP_API_KEY }}
          PULSEMCP_TENANT_ID: ${{ secrets.PULSEMCP_TENANT_ID }}
        run: python -m ${{ matrix.module }} --output-dir data/${{ matrix.name }}/ --mode ${{ env.CRAWL_MODE }}

      - name: Check for changes
        id: changes
        run: |
          if [ -f "data/${{ matrix.name }}/.changed_files.txt" ]; then
            COUNT=$(wc -l < "data/${{ matrix.name }}/.changed_files.txt" | tr -d ' ')
            echo "changed=$COUNT" >> "$GITHUB_OUTPUT"
            echo "has_changes=true" >> "$GITHUB_OUTPUT"
            echo "::notice::${{ matrix.name }}: $COUNT files changed"
          else
            echo "changed=0" >> "$GITHUB_OUTPUT"
            echo "has_changes=false" >> "$GITHUB_OUTPUT"
            echo "::notice::${{ matrix.name }}: no changes"
          fi

      - uses: actions/upload-artifact@v4
        with:
          name: crawl-${{ matrix.name }}
          path: data/
          retention-days: 1
          if-no-files-found: ignore

  # ─────────────────────────────────────────────
  # Phase 2: Delta scan — only changed files
  # ─────────────────────────────────────────────

  scan:
    needs: crawl
    if: ${{ always() && !cancelled() }}
    runs-on: ubuntu-latest
    timeout-minutes: 60

    steps:
      - uses: actions/checkout@v4

      - uses: actions/setup-python@v5
        with:
          python-version: "3.12"
          cache: pip

      - uses: actions/setup-go@v5
        with:
          go-version: "1.25"

      - run: pip install -r requirements.txt

      - name: Build Aguara from source
        run: |
          git clone --depth 1 https://github.com/garagon/aguara.git /tmp/aguara-src
          cd /tmp/aguara-src
          go build -o "$GITHUB_WORKSPACE/bin/aguara" ./cmd/aguara/
          chmod +x "$GITHUB_WORKSPACE/bin/aguara"
          "$GITHUB_WORKSPACE/bin/aguara" version

      - name: Download crawled files
        uses: actions/download-artifact@v4
        with:
          pattern: crawl-*
          path: data/
          merge-multiple: true

      - name: Delta scan and ingest
        env:
          TURSO_DATABASE_URL: ${{ secrets.TURSO_DATABASE_URL }}
          TURSO_AUTH_TOKEN: ${{ secrets.TURSO_AUTH_TOKEN }}
        run: |
          ANY_CHANGES=false
          for REG in skills-sh clawhub mcp-registry mcp-so lobehub; do
            DATA_DIR="data/$REG"
            MANIFEST="$DATA_DIR/.changed_files.txt"

            if [ ! -d "$DATA_DIR" ] || [ -z "$(ls -A "$DATA_DIR" 2>/dev/null)" ]; then
              echo "::notice::Skipping $REG (no data)"
              continue
            fi

            # Use delta directory if manifest exists
            SCAN_DIR="$DATA_DIR"
            if [ -f "$MANIFEST" ] && [ -s "$MANIFEST" ]; then
              DELTA_DIR="data/delta-$REG"
              mkdir -p "$DELTA_DIR"
              while IFS= read -r fname; do
                [ -z "$fname" ] && continue
                if [ -f "$DATA_DIR/$fname" ]; then
                  ln -sf "$(realpath "$DATA_DIR/$fname")" "$DELTA_DIR/$fname"
                fi
              done < "$MANIFEST"
              SCAN_DIR="$DELTA_DIR"
              COUNT=$(wc -l < "$MANIFEST" | tr -d ' ')
              echo "::notice::$REG: scanning $COUNT changed files (delta)"
              ANY_CHANGES=true
            else
              echo "::notice::$REG: no manifest, running full scan"
            fi

            echo "::group::Scanning $REG"
            RESULTS="data/${REG}-results.json"
            ./bin/aguara scan "$SCAN_DIR" --format json > "$RESULTS" 2>/dev/null || true
            python -m scanner.ingest "$RESULTS" --registry "$REG" --delta
            echo "::endgroup::"
          done

          echo "any_changes=$ANY_CHANGES" >> "$GITHUB_OUTPUT"

  # ─────────────────────────────────────────────
  # Phase 3: Aggregate + publish (if changes)
  # ─────────────────────────────────────────────

  aggregate:
    needs: scan
    if: ${{ always() && !cancelled() && needs.scan.result == 'success' }}
    runs-on: ubuntu-latest
    timeout-minutes: 30

    steps:
      - uses: actions/checkout@v4

      - uses: actions/setup-python@v5
        with:
          python-version: "3.12"
          cache: pip

      - run: pip install -r requirements.txt

      - name: Compute daily stats
        env:
          TURSO_DATABASE_URL: ${{ secrets.TURSO_DATABASE_URL }}
          TURSO_AUTH_TOKEN: ${{ secrets.TURSO_AUTH_TOKEN }}
        run: python -m aggregator.stats

      - name: Recompute skill scores
        env:
          TURSO_DATABASE_URL: ${{ secrets.TURSO_DATABASE_URL }}
          TURSO_AUTH_TOKEN: ${{ secrets.TURSO_AUTH_TOKEN }}
        run: python -m aggregator.scores

      - name: Export static API + datasets
        env:
          TURSO_DATABASE_URL: ${{ secrets.TURSO_DATABASE_URL }}
          TURSO_AUTH_TOKEN: ${{ secrets.TURSO_AUTH_TOKEN }}
        run: python -m aggregator.export --output-dir web/public/api/v1 --datasets-dir web/public/datasets

      - uses: actions/upload-artifact@v4
        with:
          name: api-data
          path: |
            web/public/api/
            web/public/datasets/
          retention-days: 7

  publish:
    needs: aggregate
    if: ${{ always() && !cancelled() && needs.aggregate.result == 'success' }}
    runs-on: ubuntu-latest
    timeout-minutes: 30

    steps:
      - uses: actions/checkout@v4

      - name: Download API data
        uses: actions/download-artifact@v4
        with:
          name: api-data
          path: web/public/

      - uses: actions/setup-node@v4
        with:
          node-version: "20"

      - name: Install & build Astro
        working-directory: web
        run: npm ci && npm run build

      - name: Deploy to Netlify
        uses: nwtgck/actions-netlify@v3
        with:
          publish-dir: web/dist
          production-deploy: true
          deploy-message: "Incremental crawl - ${{ github.sha }}"
        env:
          NETLIFY_AUTH_TOKEN: ${{ secrets.NETLIFY_AUTH_TOKEN }}
          NETLIFY_SITE_ID: ${{ secrets.NETLIFY_SITE_ID }}
